{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "44c23564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d240312b",
   "metadata": {},
   "source": [
    "DataFrame 불러오기\n",
    "\n",
    "Data : ace 위성의 데이터, 국내 전파 센터 3곳(이전, 제주, 강릉)의 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "1826594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/competition/result/data/whole_df.csv\")\n",
    "\n",
    "\n",
    "df = df.drop(\"datetime\",axis =1)\n",
    "df = df.astype(float)\n",
    "\n",
    "\n",
    "df.columns = [\"datetime\", \"proton_density\", \"proton_temperature\", \"proton_speed\",\"bx_gsm\",\"by_gsm\",\"bz_gsm\",\"bt\"]\n",
    "df.drop([\"datetime\", \"proton_density\", \"proton_temperature\", \"proton_speed\"],axis =1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44eb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 및 첨도, 왜도, describe를 통한 기본적 데이터 탐색\n",
    "df.isnull().sum(),df.kurt(),df.skew(),df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb639ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#이천, 제주, 강릉 데이터 불러오기\n",
    "#Datatime 과 DOY 는 시간 데이터이므로 drop\n",
    "\n",
    "df_K_ich = pd.read_csv(\"/home/competition/result/data/whole_df_K_ich.csv\",index_col=False)\n",
    "df_K_ich.drop([\"Unnamed: 0\",\"datetime\", \"DOY\"],axis=1,inplace = True)\n",
    "df_K_ich[\"sq\"] = np.sqrt(df_K_ich[\"X\"]**2 + df_K_ich[\"Y\"]**2 + df_K_ich[\"Z\"]**2)\n",
    "\n",
    "df_K_jj = pd.read_csv(\"/home/competition/result/data/whole_df_K_jj.csv\",index_col=False)\n",
    "df_K_jj.drop([\"Unnamed: 0\",\"datetime\", \"DOY\"],axis=1,inplace = True)\n",
    "df_K_jj[\"sq\"] = np.sqrt(df_K_jj[\"X\"]**2 + df_K_jj[\"Y\"]**2 + df_K_jj[\"Z\"]**2)\n",
    "\n",
    "\n",
    "df_K_gang = pd.read_csv(\"/home/competition/result/data/whole_df_K_gang.csv\",index_col=False)\n",
    "df_K_gang.drop([\"Unnamed: 0\",\"datetime\", \"DOY\"],axis=1,inplace = True)\n",
    "df_K_gang[\"sq\"] = np.sqrt(df_K_gang[\"X\"]**2 + df_K_gang[\"Y\"]**2 + df_K_gang[\"Z\"]**2)\n",
    "\n",
    "\n",
    "\n",
    "df_whole = pd.concat([df,df_K_jj,df_K_ich,df_K_gang],axis =1)\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "df_whole.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aadf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip을 통한 상한선, 하한선 밖의 데이터 cliping\n",
    "# 각 상한선 및 하한선은 IQR의 2.5배로 설정\n",
    "Q3 = df_whole.quantile(0.75) \n",
    "Q1 = df_whole.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_line = Q1-IQR*2.5\n",
    "upper_line = Q3+IQR*2.5\n",
    "\n",
    "\n",
    "df_whole.clip(lower = lower_line, upper = upper_line, axis=1, inplace = True)\n",
    "df_whole\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 데이터 탐색을 한 결과, 이상치가 많이 존재했으므로 이상치에 둔감한 QuantileTransformer 적용.\n",
    "# 1000분위수로 적용함.\n",
    "from sklearn.preprocessing import  QuantileTransformer\n",
    "\n",
    "scaler_ = QuantileTransformer(output_distribution=\"normal\", n_quantiles=1000)\n",
    "scaler_.fit(df_whole)\n",
    "X=scaler_.fit_transform(df_whole)\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "X, X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae92a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시계열 데이터를 만들기 위해, 180개의 분 데이터(3시간)을 하나의 array로 설정\n",
    "# 만약 데이터 중간에 null 값이 있다면 데이터 삭제\n",
    "def make_dataset(data,window_size=180):\n",
    "    feature_list = []\n",
    "    null_list = []\n",
    "    for i in range(0,len(data),180):\n",
    "        do = True\n",
    "        for a in range(i,i+180):\n",
    "            if data.loc[a].isnull().any() == True:\n",
    "                do = False\n",
    "                break\n",
    "        if do:\n",
    "            feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        else:\n",
    "            null_list.append(i/180)\n",
    "        \n",
    "    return np.array(feature_list),null_list\n",
    "X,null_list = make_dataset(X,180)\n",
    "X,len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 값 불러오기\n",
    "y = pd.read_csv(\"/home/competition/result/data/whole_y.csv\",index_col = 0)\n",
    "y = y.reset_index(drop = True)\n",
    "y,len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float 형태로 불러와 max, min 값 확인\n",
    "y =y.drop(null_list)\n",
    "y[\"0\"] = y[\"0\"].astype(float)\n",
    "soft_max = y.max()\n",
    "soft_min = y.min()\n",
    "print(y.max(),y.min())\n",
    "\"\"\"\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaler.fit(y)\n",
    "y=y_scaler.fit_transform(y)\n",
    "\"\"\"\n",
    "y,len(y),y.max(),y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36db84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 과 valid 을 0.2 비율로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,random_state =5)\n",
    "x_train.shape, x_valid.shape,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "13857ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링\n",
    "# 첫 layer는 LSTM 을 적용하고 나머지는 Dense Layer 적용\n",
    "# LSTM 은 tanh, Dense 는 relu 적용\n",
    "# 각 Dense layer는 he_normal 로 initalization\n",
    "# regularizer 는 l2로 0.01 적용\n",
    "# 이후 Dropout 0.2 적용\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout \n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "from keras.layers import LSTM, BatchNormalization \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "leaky_relu = tf.nn.leaky_relu \n",
    "model = Sequential() \n",
    "model.add(LSTM(256, input_shape=(x_train.shape[1], x_train.shape[2]),\n",
    "               kernel_initializer='he_normal',activation = \"tanh\"\n",
    "               ))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(512,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(1024,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(2048,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(1024,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(512,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(256,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(128,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(64,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(32,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(16,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01))) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(8,activation = leaky_relu,kernel_initializer='he_normal',kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Dense(1)) \n",
    "#model.add(Dense(soft_max - soft_min+1, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ad225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 는 Adam, loss 는 mse 적용함\n",
    "# batch size 는 128, epoch은 500 적용(early stop을 이용하여 학습 도중 마무리되게 설정)\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import os \n",
    "import tensorflow as tf \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) \n",
    "loss = \"mse\"\n",
    "opt = adam#RMSprop(learning_rate=0.001, rho=0.9) \n",
    "model.compile(optimizer=opt, loss=loss, metrics=['mae']) \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "model_path = 'model' \n",
    "filename = os.path.join(model_path, 'tmp_checkpoint.h5') \n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto') \n",
    "batch_size = 128\n",
    "history = model.fit(x_train, y_train, epochs=500, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_valid, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 데이터 불러오기 및 전처리\n",
    "pred_data = pd.read_csv(\"/home/competition/result/data/ACE_DSCOVR_Satellite/solar_wind_data_verif_2.csv\")\n",
    "\n",
    "pred_data = pred_data.drop([\"time\"], axis =1)\n",
    "pred_data.columns = [\"datetime\",\"proton_density\",\"proton_temperature\",\"proton_speed\",\"bx_gsm\",\"by_gsm\",\"bz_gsm\",\"bt\"]\n",
    "pred_data.drop([\"datetime\", \"proton_density\", \"proton_temperature\", \"proton_speed\"],axis =1,inplace = True)\n",
    "\n",
    "\n",
    "pred_data = pred_data.astype(float)\n",
    "\n",
    "\n",
    "pred_jj = pd.read_csv(\"/home/competition/result/data/Ground_Geomagnetic_Data_Korea/Jeju/geomagnetic_field_data_jj_verif_2.csv\")\n",
    "pred_jj.drop([\"day\",\"time\"],axis =1 , inplace = True)\n",
    "pred_jj[\"sq\"] = np.sqrt(pred_jj[\"X\"]**2 + pred_jj[\"Y\"]**2 + pred_jj[\"Z\"]**2)\n",
    "\n",
    "pred_ich = pd.read_csv(\"/home/competition/result/data/Ground_Geomagnetic_Data_Korea/Icheon/geomagnetic_field_data_ich_verif_2.csv\")\n",
    "pred_ich.drop([\"day\",\"time\"],axis =1 , inplace = True)\n",
    "pred_ich[\"sq\"] = np.sqrt(pred_ich[\"X\"]**2 + pred_ich[\"Y\"]**2 + pred_ich[\"Z\"]**2)\n",
    "\n",
    "\n",
    "pred_gang = pd.read_csv(\"/home/competition/result/data/Ground_Geomagnetic_Data_Korea/Gangneung/geomagnetic_field_data_gn_verif_2.csv\")\n",
    "pred_gang.drop([\"day\",\"time\"],axis =1 , inplace = True)\n",
    "pred_gang[\"sq\"] = np.sqrt(pred_gang[\"X\"]**2 + pred_gang[\"Y\"]**2 + pred_gang[\"Z\"]**2)\n",
    "\n",
    "\n",
    "\n",
    "pred_data_whole = pd.concat([pred_data,pred_jj,pred_ich, pred_gang], axis=1)\n",
    "\n",
    "for a in pred_data_whole.columns:\n",
    "    print(a,pred_data_whole[a].kurt(),pred_data_whole[a].skew())\n",
    "\n",
    "\n",
    "#pred_data_whole.clip(lower = lower_line, upper = upper_line, axis=1,inplace = True)\n",
    "print(pred_data_whole.isnull().sum())\n",
    "pred_data_whole = scaler_.fit_transform(pred_data_whole)\n",
    "pred_data_whole = pd.DataFrame(pred_data_whole)\n",
    "pred_data_whole = pred_data_whole.interpolate(method = \"pad\",limit =5000)\n",
    "for a in pred_data_whole.columns:\n",
    "    print(a,pred_data_whole[a].kurt(),pred_data_whole[a].skew())\n",
    "\n",
    "pred_data_whole.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "c0c78397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_make_dataset(data,window_size=180):\n",
    "    feature_list = []\n",
    "    \n",
    "    for i in range(0,len(data),180):\n",
    "        \n",
    "            feature_list.append(np.array(data.iloc[i:i+window_size]))\n",
    "        \n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "66fb1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "pred = pred_make_dataset(pred_data_whole,180)\n",
    "from keras.models import load_model\n",
    "model = load_model('model/tmp_checkpoint_0-Copy1.61_layer256~2048_mse_128_last.h5', custom_objects={'root_mean_squared_error': root_mean_squared_error})\n",
    "prediction = model.predict(pred)\n",
    "#result = y_scaler.inverse_transform(pd.DataFrame(prediction))\n",
    "\n",
    "result = pd.DataFrame(prediction).interpolate(method = \"pad\",limit =9)\n",
    "res = result.iloc[240:].round(0)\n",
    "res.to_csv(\"result_lastest.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "aeb5b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = []\n",
    "for a in range(len(res)):\n",
    "    now=res.iloc[a,:].to_list()\n",
    "    res_list.append((now.index(max(now))))\n",
    "res_list = pd.DataFrame(res_list)\n",
    "res_list.to_csv(f\"result_soft_max_{loss}_{opt}_{batch_size}_lasttest.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb997a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea6eac05f6a8337642931beb9ca9c413554dc63ec7b1c1e10f71e0c07b5a2fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
